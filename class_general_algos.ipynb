{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_descent:\n",
    "    def __init__(self, error):\n",
    "        self.x_original = None\n",
    "        self.a_original = None\n",
    "        self.b_original = None\n",
    "        self.x = None\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        self.initialTime = None\n",
    "        self.error = error\n",
    "        self.grad = None\n",
    "        self.lam = None\n",
    "        self.gr = None\n",
    "        self.theta = None\n",
    "    \n",
    "    def generate(self, function, dim, vec_len, scale):\n",
    "        for i in range(dim):\n",
    "            globals()['X'+str(i)] = [0]*vec_len\n",
    "        for j in range(vec_len):\n",
    "            globals()['X'+str(0)][j] = (random.random()+1e-12)*scale\n",
    "            y = function(globals()['X'+str(0)][j])\n",
    "            noiz = 3\n",
    "            for k in range(dim):\n",
    "                globals()['X'+str(k)][j] = y + np.random.normal(0,noiz)\n",
    "                noiz += 3\n",
    "        A = np.array([globals()['X'+str(i)] for i in range(dim)]).T\n",
    "        self.a_original = A\n",
    "        self.x_original = np.random.rand(len(A[0]))*scale\n",
    "        self.b_original = A @ self.x_original + np.random.normal(0,3,(A @ self.x_original).shape)\n",
    "    def LinRegFunc(self, X):\n",
    "        return 0.5 * np.linalg.norm(self.a @ X - self.b)**2\n",
    "    def LinRegGrad(self, X):\n",
    "        return self.a.T @ (self.a @ X - self.b)\n",
    "    def Step(self):\n",
    "        self.x = self.x - self.lam*self.grad\n",
    "    def BackTrack(self):\n",
    "        alpha = float(random.getrandbits(8))\n",
    "        rho = random.random()+1e-12\n",
    "        c = random.random()+1e-12\n",
    "        while self.LinRegFunc(self.x - alpha*self.grad) > self.LinRegFunc(self.x) \\\n",
    "                - c*alpha*np.linalg.norm(self.grad)**2:\n",
    "            alpha = rho*alpha\n",
    "        return alpha\n",
    "    \n",
    "    def Vanilla(self):\n",
    "        self.initialTime = time.time()\n",
    "        self.x = deepcopy(self.x_original)\n",
    "        self.a = deepcopy(self.a_original)\n",
    "        self.b = deepcopy(self.b_original)\n",
    "        self.gr = []\n",
    "        L = np.max( np.linalg.svd( self.a @ self.a.T ) [1] )\n",
    "        self.lam = 1/L\n",
    "        self.grad = self.LinRegGrad(self.x)\n",
    "        while np.linalg.norm(self.grad) > self.error:\n",
    "            self.Step()\n",
    "            self.grad = self.LinRegGrad(self.x)\n",
    "            self.gr.append(np.linalg.norm(self.grad))\n",
    "        print(' \\n Vanilla Gradient Descent:')\n",
    "        print('_____________________________________________')\n",
    "        self.PrintPlots(0, len(self.gr))\n",
    "        self.PrintResults()\n",
    "    \n",
    "    def Adaptive(self):\n",
    "        self.initialTime = time.time()\n",
    "        self.x = deepcopy(self.x_original)\n",
    "        self.a = deepcopy(self.a_original)\n",
    "        self.b = deepcopy(self.b_original)\n",
    "        self.gr = []\n",
    "        self.lam = random.random()+1e-12\n",
    "        self.theta = float(random.getrandbits(128))\n",
    "        oldX = deepcopy(self.x)\n",
    "        self.grad = self.LinRegGrad(self.x)\n",
    "        self.Step()\n",
    "        while np.linalg.norm(self.grad) > self.error:\n",
    "            oldLam = deepcopy(self.lam)\n",
    "            min1 = np.sqrt(1 + self.theta)*self.lam\n",
    "            min2 = ( np.linalg.norm( self.x - oldX ) ) \\\n",
    "                / ( 2 * np.linalg.norm( self.LinRegGrad(self.x) - self.LinRegGrad(oldX) ) )\n",
    "            self.lam = np.min([min1,min2])\n",
    "            oldX = deepcopy(self.x)\n",
    "            self.Step()\n",
    "            self.theta = self.lam/oldLam\n",
    "            self.grad = self.LinRegGrad(self.x)\n",
    "            self.gr.append(np.linalg.norm(self.grad))\n",
    "        print('\\n Adaptive Gradient Descent:')\n",
    "        print('_____________________________________________')\n",
    "        self.PrintPlots(0, int(np.ceil(0.1*len(self.gr))))\n",
    "        self.PrintPlots(int(np.ceil(0.1*len(self.gr))), int(np.ceil(0.4*len(self.gr))))\n",
    "        self.PrintPlots(int(np.ceil(0.4*len(self.gr))), len(self.gr))\n",
    "        self.PrintResults()\n",
    "    \n",
    "    def LineSearch(self):\n",
    "        self.initialTime = time.time()\n",
    "        self.x = deepcopy(self.x_original)\n",
    "        self.a = deepcopy(self.a_original)\n",
    "        self.b = deepcopy(self.b_original)\n",
    "        self.gr = []\n",
    "        self.grad = self.LinRegGrad(self.x)\n",
    "        self.lam = self.BackTrack()\n",
    "        while np.linalg.norm(self.grad) > self.error:\n",
    "            self.Step()\n",
    "            self.grad = self.LinRegGrad(self.x)\n",
    "            self.gr.append(np.linalg.norm(self.grad))\n",
    "            self.lam = self.BackTrack()\n",
    "        print('\\n Line Search Gradient Descent:')\n",
    "        print('_____________________________________________')\n",
    "        self.PrintPlots(0, int(np.ceil(0.05*len(self.gr))))\n",
    "        self.PrintPlots(int(np.ceil(0.05*len(self.gr))), int(np.ceil(0.25*len(self.gr))))\n",
    "        self.PrintPlots(int(np.ceil(0.25*len(self.gr))), len(self.gr))\n",
    "        self.PrintResults()\n",
    "    \n",
    "    def Barzilai(self):\n",
    "        self.initialTime = time.time()\n",
    "        self.x = deepcopy(self.x_original)\n",
    "        self.a = deepcopy(self.a_original)\n",
    "        self.b = deepcopy(self.b_original)\n",
    "        self.gr = []\n",
    "        self.grad = self.LinRegGrad(self.x)\n",
    "        self.lam = random.random()+1e-12\n",
    "        oldX = deepcopy(self.x)\n",
    "        self.Step()\n",
    "        while np.linalg.norm(self.grad) > self.error:\n",
    "            oldLam = deepcopy(self.lam)\n",
    "            self.lam = np.dot( self.x - oldX, self.LinRegGrad(self.x) - self.LinRegGrad(oldX) ) \\\n",
    "                        / np.linalg.norm(self.LinRegGrad(self.x) - self.LinRegGrad(oldX))**2\n",
    "            oldX = deepcopy(self.x)\n",
    "            self.Step()\n",
    "            self.grad = self.LinRegGrad(self.x)\n",
    "            self.gr.append(np.linalg.norm(self.grad))\n",
    "        print('\\n Barzilai-Borwein Gradient Descent:')\n",
    "        print('_____________________________________________')\n",
    "        self.PrintPlots(0, len(self.gr))\n",
    "        self.PrintResults()\n",
    "    \n",
    "    def PrintPlots(self, Min, Max):\n",
    "        plt.plot( range( Min, Max ) , self.gr[Min:Max] , c = 'g' )\n",
    "        plt.title(label = 'Norm of Gradient in domain [{}, {}]'.format(Min,Max) )\n",
    "        plt.show()\n",
    "    def PrintResults(self):\n",
    "        print('At final iteration k =', len(self.gr), ':')\n",
    "        print('x vector is:', self.x)\n",
    "        print('Gradient Norm is:', np.linalg.norm(self.grad))\n",
    "        print('Time taken:', time.time() - self.initialTime)\n",
    "        print('Final Lambda value: ', self.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linFunc(x):\n",
    "    return 2*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gradient_descent(1e-3)\n",
    "G.generate(linFunc,2,100, 10)\n",
    "G.Vanilla()\n",
    "G.LineSearch()\n",
    "G.Adaptive()\n",
    "G.Barzilai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    if t < -700:\n",
    "        return 0\n",
    "    else:\n",
    "        return (1 + np.e**(-t))**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-701)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logReg(a, x, b):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = {}\n",
    "# for i in range(100):\n",
    "#     X[i] = [0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimes = np.array(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "for i in range(100):\n",
    "    globals()['x'+str(i)] = k\n",
    "    k = k*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    globals()['X'+str(i)] = [0]*10\n",
    "for j in range(10):\n",
    "    globals()['X'+str(0)][j] = (random.random()+1e-12)*10\n",
    "    y = 2*globals()['X'+str(0)][j] + 5\n",
    "    noiz = 3\n",
    "    for k in range(10):\n",
    "        globals()['X'+str(k)][j] = y + np.random.normal(0,noiz)\n",
    "        noiz += 3\n",
    "A = [globals()['X'+str(i)] for i in range(10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
